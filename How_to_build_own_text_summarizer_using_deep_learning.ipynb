{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFuL-RBgXqgU"
   },
   "source": [
    "In this notebook, we will build an abstractive based text summarizer using deep learning from the scratch in python using keras\n",
    "\n",
    "I recommend you to go through the article over [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) to cover all the concepts which is required to build our own summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F5dSoP8lGMZi"
   },
   "source": [
    "#Understanding the Problem Statement\n",
    "\n",
    "Customer reviews can often be long and descriptive. Analyzing these reviews manually, as you can imagine, is really time-consuming. This is where the brilliance of Natural Language Processing can be applied to generate a summary for long reviews.\n",
    "\n",
    "We will be working on a really cool dataset. Our objective here is to generate a summary for the Amazon Fine Food reviews using the abstraction-based approach we learned about above. You can download the dataset from[ here ](https://www.kaggle.com/snap/amazon-fine-food-reviews)\n",
    "\n",
    "It’s time to fire up our Jupyter notebooks! Let’s dive into the implementation details right away.\n",
    "\n",
    "#Custom Attention Layer\n",
    "\n",
    "Keras does not officially support attention layer. So, we can either implement our own attention layer or use a third-party implementation. We will go with the latter option for this article. You can download the attention layer from [here](https://github.com/thushv89/attention_keras/blob/master/layers/attention.py) and copy it in a different file called attention.py.\n",
    "\n",
    "Let’s import it into our environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi64aA0FFxcS"
   },
   "outputs": [],
   "source": [
    "from attention import AttentionLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JUValOzcHtEK"
   },
   "source": [
    "#Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {},
    "colab_type": "code",
    "id": "_Jpu8qLEFxcY",
    "outputId": "95968e01-faac-4911-c802-9c008a4e62cf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVakjZ3oICgx"
   },
   "source": [
    "#Read the dataset\n",
    "\n",
    "This dataset consists of reviews of fine foods from Amazon. The data spans a period of more than 10 years, including all ~500,000 reviews up to October 2012. These reviews include product and user information, ratings, plain text review, and summary. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "We’ll take a sample of 100,000 reviews to reduce the training time of our model. Feel free to use the entire dataset for training your model if your machine has that kind of computational power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnK5o4Z1Fxcj"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"./input/amazon-fine-food-reviews/articles2.csv\",nrows=50000)\n",
    "#data=pd.read_csv(\"./input/amazon-fine-food-reviews/Reviews.csv\",nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kGNQKvCaISIn"
   },
   "source": [
    "# Drop Duplicates and NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cjul88oOFxcr"
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['content'],inplace=True)#dropping duplicates\n",
    "data.dropna(axis=0,inplace=True)#dropping na"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi0xD6BkIWAm"
   },
   "source": [
    "# Information about dataset\n",
    "\n",
    "Let us look at datatypes and shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__fy-JxTFxc9",
    "outputId": "d42c6e36-bbc8-43c2-de0e-d3effe3e8c4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33346 entries, 7012 to 49998\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Unnamed: 0   33346 non-null  int64  \n",
      " 1   id           33346 non-null  int64  \n",
      " 2   title        33346 non-null  object \n",
      " 3   publication  33346 non-null  object \n",
      " 4   author       33346 non-null  object \n",
      " 5   date         33346 non-null  object \n",
      " 6   year         33346 non-null  float64\n",
      " 7   month        33346 non-null  float64\n",
      " 8   url          33346 non-null  object \n",
      " 9   content      33346 non-null  object \n",
      "dtypes: float64(2), int64(2), object(6)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0xLYACiFxdJ"
   },
   "source": [
    "#Preprocessing\n",
    "\n",
    "Performing basic preprocessing steps is very important before we get to the model building part. Using messy and uncleaned text data is a potentially disastrous move. So in this step, we will drop all the unwanted symbols, characters, etc. from the text that do not affect the objective of our problem.\n",
    "\n",
    "Here is the dictionary that we will use for expanding the contractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0s6IY-x2FxdL"
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2JFRXFHmI7Mj"
   },
   "source": [
    "We will perform the below preprocessing tasks for our data:\n",
    "\n",
    "1.Convert everything to lowercase\n",
    "\n",
    "2.Remove HTML tags\n",
    "\n",
    "3.Contraction mapping\n",
    "\n",
    "4.Remove (‘s)\n",
    "\n",
    "5.Remove any text inside the parenthesis ( )\n",
    "\n",
    "6.Eliminate punctuations and special characters\n",
    "\n",
    "7.Remove stopwords\n",
    "\n",
    "8.Remove short words\n",
    "\n",
    "Let’s define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZr-u3OEFxdT"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def text_cleaner(text,num):\n",
    "    newString = text.lower()\n",
    "    newString = BeautifulSoup(newString, \"lxml\").text\n",
    "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
    "    newString = re.sub('\"','', newString)\n",
    "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])    \n",
    "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
    "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \n",
    "    newString = re.sub('[m]{2,}', 'mm', newString)\n",
    "    if(num==0):\n",
    "        tokens = [w for w in newString.split() if not w in stop_words]\n",
    "    else:\n",
    "        tokens=newString.split()\n",
    "    long_words=[]\n",
    "    for i in tokens:\n",
    "        if len(i)>1:                                                 #removing short word\n",
    "            long_words.append(i)   \n",
    "    return (\" \".join(long_words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2QAeCHWFxdY"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_text = []\n",
    "for t in data['content']:\n",
    "    cleaned_text.append(text_cleaner(t,0)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snRZY8wjLao2"
   },
   "source": [
    "Let us look at the first five preprocessed reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCAIkhWbFxdh",
    "outputId": "c2da1a36-4488-4e32-ef9e-fcfe496e374d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['donald trump launched new attacks bill hillary clinton wednesday war words campaigns heated gop hopeful told supporters hilton head island forced fight back clinton camp democratic frontrunner accused displaying penchant sexism trump blasted former president saying hillary wants accuse things husband one great abusers world give break give break give break came saying trump penchant sexism playing card trump explained choice mention husband situation reference bill clinton previous extramarital relationships tuesday trump told reporters bill clinton past play election hillary brought whole thing calling trump sexist said got major problem happens right house go come well us trump also acknowledged personal indiscretions including affair marla maples later became second wife fair game clintons trumps friendly terms years clintons attended trump wedding third wife melania couples daughters ivanka chelsea friends trump came bill clinton defense monica lewinsky scandal unfolding calling efforts impeach nonsense recent days rival campaigns become increasingly hostile trump first stirred controversy michigan rally dec claimed clinton got schlonged iowa caucuses barack obama clinton made penchant sexism claim response real estate mogul statement trump said remark meant vulgar clinton deputy communications director christina reynolds said monday clinton bullied trump plans stand beginning campaign insults women groups effort shape clinton image electorate wednesday trump called former secretary state label previously reserved former florida governor jeb bush went suggesting women want vote clinton drew loud cheers crowd win said trump added would love love woman president hillary clinton described horrible hard listen turn television many times gives headache said clinton leads trump percent percent latest real clear politics average recent polls trump predicted general election matchup clinton could lead one largest voter turnouts recent history associated press contributed report chris snyder producer foxnews com based new york follow twitter chrissnyderfox',\n",
       " 'plunge oil prices given needed break drivers holiday season causing real pain states rely oil revenue fuel economies shore budgets perhaps nowhere impact pronounced alaska gov bill walker proposing raft new taxes including first personal income tax three decades along budget cuts offset damage price drop state major paradigm shift state alaska conducts business walker said statement announced plan december cannot continue business usual live solely natural resource revenues price brent crude united states fallen barrel lower may alaska state reliant accustomed big oil revenue residents share profits sign changing times walker plan would redirect money government making smaller dividend checks residents according walker administration income tax component new sustainable alaska plan could generate million revenue year plan average alaskan family would pay rate roughly percent gross income would coincide cuts everything education programs grants emergency communication never state faced deficit large draining million savings every day walker said statement fortunately came us wisdom set aside money rainy day well raining given financial straits government walker independent garnered bipartisan support lawmakers still faces reluctance pursuing income tax statement house operating budget chairman mark neuman republican said walker deserves credit proposing difficult options filling income gap still said plan could use budget cuts house capital budget chairman steve thompson also republican echoed critique said want residents pay income tax unless absolutely necessary plan taxes oil industries also would increase would alcohol tobacco taxes alaska vulnerable position big oil state like texas enjoys diverse economy chris bryan spokesman texas comptroller public accounts told fox news state projecting lower tax revenue state diverse economy coupled large beginning balance conservative budget texas legislature allow state absorb reduction projected revenues said government still predicting economic growth texas north percent fiscal yet north dakota oil gas revolution transformed state drop prices also threatens significant damage recent moody analytics study reportedly said state could nearing recession citing oil price north dakota lowest since according watchdog org report north dakota general fund tax revenue million percent less forecasted lawmakers seem like revenues going rebound near term state sen gary lee republican told watchdog org according sheila peterson director fiscal management division north dakota office management budget falling oil prices crunching budget much alaska direct oil revenue goes general fund million billion budget peterson told fox news still expect get million direct oil taxes according north dakota omb oil tax composes percent north dakota general fund revenue north dakota runs budget updated revenue forecasts january although revenues indeed running forecast right though run money peterson said depending next forecast shows decide need take action actions',\n",
       " 'concealed handgun permit holders texas good chance someone next lone star state grocery store restaurant carrying concealed handgun starting friday texas join states already allow people openly carry handguns throughout state amidst today threats terrorism mass public shootings good thing enable people legally carry guns protect new law person need concealed handgun permit able openly carry gun experiences states guide people actually openly carry handguns moreover businesses still prohibit guns request concealed however chief financial officer kroger said company allow open carry says company encountered problems open carry supermarket companies whole foods randall posting signs banning open carry still allowing concealed carry bans may short lived texas originally passed concealed carry many stores initially posted signs banning concealed carry years signs disappeared odd nuance texas law led much national attention last couple years friday people allowed openly carry rifles handguns problems ever occurred simply additional handling required carrying rifle opposed keeping holstered handgun certainly created concern something might go wrong michael bloomberg everytown advocacy group put pressure companies ban openly carried rifles hillary clinton also chimed idea could open carry permit shoulder walking isles supermarket despicable starbucks jack box chipotle wendy applebee chili sonic respectfully request customers openly carry guns importantly however still allow people carry concealed gun openly carrying handguns less threatening rifles likely face greater issues states open carry important drawback effective concealed carry protecting people terrorist attacks mass public shootings criminals terrorists strike anywhere time attack someone openly carrying gun alternatively select another target wait opportune moment concealed carry effective way counteracting strategic advantage killer attack big grocery store texas without facing likely resistance course attacker idea might packing heat time time mass public shooters openly admit targeting zones since least two mass public shootings occurred zone policeone largest private organization police officers us recently asked members considering particulars recent mass shooting tragedies like newtown aurora level impact think citizen could made eighty percent said casualties would likely reduced million american civilians licensed carry concealed handguns every day permit holders stop crimes also stopped large growing number mass public shootings concealed carry texas common may think percent state adult population permits rate twice high pennsylvania florida also large states simple reason texas charges permit fee one highest country lower fees would increase number concealed carry permits number people protect others would especially help likely victims violent crime poor black men women living urban areas year predict people wonder fuss made open carry michael bloomberg may scored temporary victories open carry fear tactics discredited john lott jr columnist foxnews com economist formerly chief economist united states sentencing commission lott also leading expert guns issue done conjunction crime prevention research center author eight books including guns less crime latest book dumbing courts politics keeps smartest judges bench bascom hill publishing group follow twitter johnrlottjr foxnewsopinion facebook',\n",
       " 'republican presidential candidates attacking president obama plan use oval office powers try tighten laws arguing efforts unconstitutional another attempt sidestep congress changing second amendment donald trump said saturday campaign rally biloxi miss veto fast obama said weekend meet monday attorney general loretta lynch discuss options tightening federal firearms laws reduce gun violence instructing white house team several months ago look type action could take president petulant child gop candidate new jersey gov chris christie told fox news sunday whenever get wants president acts like king obama purportedly use executive action require gun sellers order background checks prospective buyers tighten laws gun sales committed offenses president pattern taking away rights citizens gop candidate former florida gov jeb bush told fox news sunday bush also suggested object principle keeping guns hands criminals wary burdensome proposed changes might gun sellers know asked better approach would punish people violate federal gun laws great idea let go congress president tried unsuccessfully aftermath mass shooting sandy hook elementary school newtown conn get congress pass comprehensive gun control legislation national rifle association opposed plan also opposes new plan calling political stunt christie others point obama tried use executive action allow illegal immigrants remain united states work however federal appeals court temporarily stopped action pending final ruling sure get stopped courts christie also said sunday trump billionaire businessman sunday told cbs face nation elected would use executive powers repeal obama likely new executive orders gun control one thing good executive orders new president comes boom first day first hour first minute rescind said gop candidate carly fiorina told cnn state union obama lawless president use executive orders delusional dangerous mention unconstitutional said long lists criminals guns routinely purchase guns know people prosecuting three democratic presidential candidates hillary clinton former maryland gov martin malley vermont sen bernie sanders support tighter gun control sander told abc week wishes obama could get bipartisan congressional support supports president renewed efforts wide consensus overwhelming majority american people believe expand strengthen instant background checks said think president trying think right thing sanders also said supports background check would likely help close advocates call gun show loophole well strong measures keep criminals people mental health issues owning firearms',\n",
       " 'president obama plotting attorney general get guns president purportedly bypass congress crack small scale gun sellers fox news reports plan would require gun sellers order background checks prospective buyers tighten laws gun sales committed offenses click sign todd american dispatch conservatives white house really wants crack gun violence maybe enforce laws already books point president ultimately wants disarm nation primary reason founding fathers wrote second amendment protect amendments muslim terrorist attack san bernardino washington post found percent voters oppose ban assault weapons record high american people seem understand president guns keep families safe instead declaring war gun owners maybe president ought declare war true threat facing nation radical islam national rifle association accused president pulling political stunt republican presidential candidates widely condemned president plans president petulant child new jersey gov chris christie told fox news sunday whenever get wants president acts like king carly fiorina told cnn president lawless president delusional dangerous mention unconstitutional said long list criminals guns routinely purchase guns know people prosecuting texas governor greg abbott summed best tweet obama wants impose gun control response come take todd starnes host fox news commentary heard hundreds radio stations latest book god less america real stories front lines attack traditional values follow todd twitter toddstarnes find facebook foxnewsopinion facebook']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_text[:5]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GsRXocxoFxd-"
   },
   "outputs": [],
   "source": [
    "#call the function\n",
    "cleaned_summary = []\n",
    "for t in data['title']:\n",
    "    cleaned_summary.append(text_cleaner(t,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZeD0gs6Lnb-"
   },
   "source": [
    "Let us look at the first 10 preprocessed summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jQJdZcAzFxee",
    "outputId": "a1fbe683-c03f-4afb-addf-e075021c121b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['donald trump blasts bill clinton as one of the great abusers of the world',\n",
       " 'drop in oil prices rocks producer states triggers historic tax hike plan in alaska',\n",
       " 'open carry comes to texas why the lone star state will be safer in',\n",
       " 'gop field rips obama move toward executive action to tighten gun control laws',\n",
       " 'president obama wants to disarm america',\n",
       " 'rancher family reports to prison does not endorse oregon siege',\n",
       " 'our shia problem',\n",
       " 'country music saddest stories of',\n",
       " 'obama small ball on guns could be big deal',\n",
       " 'martha maccallum gun violence and tears for lost children']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_summary[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1zLpnqsFxey"
   },
   "outputs": [],
   "source": [
    "data['cleaned_text']=cleaned_text\n",
    "data['cleaned_summary']=cleaned_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KT_D2cLiLy77"
   },
   "source": [
    "#Drop empty rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYK390unFxfA"
   },
   "outputs": [],
   "source": [
    "data.replace('', np.nan, inplace=True)\n",
    "data.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vm8Fk2TCL7Sp"
   },
   "source": [
    "#Understanding the distribution of the sequences\n",
    "\n",
    "Here, we will analyze the length of the reviews and the summary to get an overall idea about the distribution of length of the text. This will help us fix the maximum length of the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdF76AHHFxgw",
    "outputId": "e3bbe165-4235-482f-bfd4-36a3f1d95290"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_word_count = []\n",
    "summary_word_count = []\n",
    "\n",
    "# populate the lists with sentence lengths\n",
    "for i in data['cleaned_text']:\n",
    "      text_word_count.append(len(i.split()))\n",
    "\n",
    "for i in data['cleaned_summary']:\n",
    "      summary_word_count.append(len(i.split()))\n",
    "\n",
    "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
    "\n",
    "length_df.hist(bins = 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QwdSGIhGMEbz"
   },
   "source": [
    "Interesting. We can fix the maximum length of the summary to 8 since that seems to be the majority summary length.\n",
    "\n",
    "Let us understand the proportion of the length of summaries below 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JRjwdIOFxg3",
    "outputId": "f968be82-c539-471d-ce23-16f18b059ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37579885385100065\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in data['cleaned_summary']:\n",
    "    if(len(i.split())<=8):\n",
    "        cnt=cnt+1\n",
    "print(cnt/len(data['cleaned_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYB4Ga9KMjEu"
   },
   "source": [
    "We observe that 94% of the summaries have length below 8. So, we can fix maximum length of summary to 8.\n",
    "\n",
    "Let us fix the maximum length of review to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKD5VOWqFxhC"
   },
   "outputs": [],
   "source": [
    "max_text_len=500\n",
    "max_summary_len=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6d48E-8M4VO"
   },
   "source": [
    "Let us select the reviews and summaries whose length falls below or equal to **max_text_len** and **max_summary_len**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yY0tEJP0FxhI"
   },
   "outputs": [],
   "source": [
    "cleaned_text =np.array(data['cleaned_text'])\n",
    "cleaned_summary=np.array(data['cleaned_summary'])\n",
    "\n",
    "short_text=[]\n",
    "short_summary=[]\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if(len(cleaned_summary[i].split())<=max_summary_len and len(cleaned_text[i].split())<=max_text_len):\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "df=pd.DataFrame({'text':short_text,'summary':short_summary})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tR1uh8xSNUma"
   },
   "source": [
    "Remember to add the **START** and **END** special tokens at the beginning and end of the summary. Here, I have chosen **sostok** and **eostok** as START and END tokens\n",
    "\n",
    "**Note:** Be sure that the chosen special tokens never appear in the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwLUH78CFxhg"
   },
   "outputs": [],
   "source": [
    "df['summary'] = df['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GlcX4RFOh13"
   },
   "source": [
    "We are getting closer to the model building part. Before that, we need to split our dataset into a training and validation set. We’ll use 90% of the dataset as the training data and evaluate the performance on the remaining 10% (holdout set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RakakKHcFxhl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vq1mqyOHOtIl"
   },
   "source": [
    "#Preparing the Tokenizer\n",
    "\n",
    "A tokenizer builds the vocabulary and converts a word sequence to an integer sequence. Go ahead and build tokenizers for text and summary:\n",
    "\n",
    "#Text Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRHTgX6hFxhq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzvLwYL_PDcx"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire text\n",
    "\n",
    "Here, I am defining the threshold to be 4 which means word whose count is below 4 is considered as a rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8KronV2Fxhx",
    "outputId": "d2eb2f27-fbbc-4e61-9556-3c3ff5e4327b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 53.062201406031484\n",
      "Total Coverage of rare words: 1.3786669346626454\n"
     ]
    }
   ],
   "source": [
    "thresh=4\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "So-J-5kzQIeO"
   },
   "source": [
    "**Remember**:\n",
    "\n",
    "\n",
    "* **tot_cnt** gives the size of vocabulary (which means every unique words in the text)\n",
    " \n",
    "*   **cnt** gives me the no. of rare words whose count falls below threshold\n",
    "\n",
    "*  **tot_cnt - cnt** gives me the top most common words \n",
    "\n",
    "Let us define the tokenizer with top most common words for reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2giEsF3Fxh3"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val   =   pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "#size of vocabulary ( +1 for padding token)\n",
    "x_voc   =  x_tokenizer.num_words + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCbGMsm4FxiA",
    "outputId": "2d9165f0-e542-4114-91f3-e070d483fce9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50009"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQfKP3sqRxi9"
   },
   "source": [
    "#Summary Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRHqyBkBFxiJ"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KInA6O6ZSkJz"
   },
   "source": [
    "#Rarewords and its Coverage\n",
    "\n",
    "Let us look at the proportion rare words and its total coverage in the entire summary\n",
    "\n",
    "Here, I am defining the threshold to be 6 which means word whose count is below 6 is considered as a rare word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yzE5OiRLFxiM",
    "outputId": "7f7a4f89-b088-4847-8172-09e5a2383d0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of rare words in vocabulary: 74.50438899299733\n",
      "Total Coverage of rare words: 10.390194165177444\n"
     ]
    }
   ],
   "source": [
    "thresh=6\n",
    "\n",
    "cnt=0\n",
    "tot_cnt=0\n",
    "freq=0\n",
    "tot_freq=0\n",
    "\n",
    "for key,value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt=tot_cnt+1\n",
    "    tot_freq=tot_freq+value\n",
    "    if(value<thresh):\n",
    "        cnt=cnt+1\n",
    "        freq=freq+value\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
    "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0PBhzKuRSw_9"
   },
   "source": [
    "Let us define the tokenizer with top most common words for summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fswLvIgFxiR"
   },
   "outputs": [],
   "source": [
    "#prepare a tokenizer for reviews on training data\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "#padding zero upto maximum length\n",
    "y_tr    =   pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val   =   pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "#size of vocabulary\n",
    "y_voc  =   y_tokenizer.num_words +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqwDUT5oTFmn"
   },
   "source": [
    "Let us check whether word count of start token is equal to length of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pR8IX9FRFxiY",
    "outputId": "b116cdbd-42c4-4ede-9f6d-46284115393e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24622, 24622)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenizer.word_counts['sostok'],len(y_tr)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVFhFVguTTtw"
   },
   "source": [
    "Here, I am deleting the rows that contain only **START** and **END** tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZ-vW82sFxih"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_tr)):\n",
    "    cnt=0\n",
    "    for j in y_tr[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr=np.delete(y_tr,ind, axis=0)\n",
    "x_tr=np.delete(x_tr,ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cx5NISuMFxik"
   },
   "outputs": [],
   "source": [
    "ind=[]\n",
    "for i in range(len(y_val)):\n",
    "    cnt=0\n",
    "    for j in y_val[i]:\n",
    "        if j!=0:\n",
    "            cnt=cnt+1\n",
    "    if(cnt==2):\n",
    "        ind.append(i)\n",
    "\n",
    "y_val=np.delete(y_val,ind, axis=0)\n",
    "x_val=np.delete(x_val,ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOtlDcthFxip"
   },
   "source": [
    "# Model building\n",
    "\n",
    "We are finally at the model building part. But before we do that, we need to familiarize ourselves with a few terms which are required prior to building the model.\n",
    "\n",
    "**Return Sequences = True**: When the return sequences parameter is set to True, LSTM produces the hidden state and cell state for every timestep\n",
    "\n",
    "**Return State = True**: When return state = True, LSTM produces the hidden state and cell state of the last timestep only\n",
    "\n",
    "**Initial State**: This is used to initialize the internal states of the LSTM for the first timestep\n",
    "\n",
    "**Stacked LSTM**: Stacked LSTM has multiple layers of LSTM stacked on top of each other. \n",
    "This leads to a better representation of the sequence. I encourage you to experiment with the multiple layers of the LSTM stacked on top of each other (it’s a great way to learn this)\n",
    "\n",
    "Here, we are building a 3 stacked LSTM for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXef38nBFxir",
    "outputId": "7ae99521-46f8-4c6f-9cba-4979deffeee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 500)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 500, 100)     5000900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 500, 100), ( 80400       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    517100      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 500, 100), ( 80400       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 100),  80400       embedding_1[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer ((None, None, 100),  20100       lstm_1[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 200)    0           lstm_2[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 5171)   1039371     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 6,818,671\n",
      "Trainable params: 6,818,671\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K \n",
    "K.clear_session()\n",
    "\n",
    "latent_dim = 100\n",
    "embedding_dim=100\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_text_len,))\n",
    "\n",
    "#embedding layer\n",
    "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
    "\n",
    "#encoder lstm 1\n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "#encoder lstm 2\n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm2(encoder_output1)\n",
    "\n",
    "#encoder lstm 3\n",
    "#encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
    "#encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "#embedding layer\n",
    "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name='attention_layer')\n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#dense layer\n",
    "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Define the model \n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZVlfRuMUcoP"
   },
   "source": [
    "I am using sparse categorical cross-entropy as the loss function since it converts the integer sequence to a one-hot vector on the fly. This overcomes any memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lwfi1Fm8Fxiz"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0ykDbxfUhyw"
   },
   "source": [
    "Remember the concept of early stopping? It is used to stop training the neural network at the right time by monitoring a user-specified metric. Here, I am monitoring the validation loss (val_loss). Our model will stop training once the validation loss increases:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-A3J92MUljB"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mw6CVECaUq5b"
   },
   "source": [
    "We’ll train the model on a batch size of 128 and validate it on the holdout set (which is 10% of our dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETnPzA4OFxi3",
    "outputId": "477e374f-7cf2-4d60-f86e-2c49c9cebedb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 138/1538 [=>............................] - ETA: 51:09 - loss: 4.8190"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-f63a2950a1d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mc:\\users\\24694\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=20,callbacks=[es],batch_size=16, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ezKYOp2UxG5"
   },
   "source": [
    "#Understanding the Diagnostic plot\n",
    "\n",
    "Now, we will plot a few diagnostic plots to understand the behavior of the model over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDTNLAURFxjE",
    "outputId": "e2ea6e44-3931-4014-97a1-03fa2a441228"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSyx-HvpUz2o"
   },
   "source": [
    "From the plot, we can infer that validation loss has increased after epoch 17 for 2 successive epochs. Hence, training is stopped at epoch 19.\n",
    "\n",
    "Next, let’s build the dictionary to convert the index to word for target and source vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBX0zZnOFxjW"
   },
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=x_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM_nU_VvFxjq"
   },
   "source": [
    "# Inference\n",
    "\n",
    "Set up the inference for the encoder and decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9QkrNV-4Fxjt"
   },
   "outputs": [],
   "source": [
    "# Encode the input sequence to get the feature vector\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Decoder setup\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs) \n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat) \n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOiyk4ToWe74"
   },
   "source": [
    "We are defining a function below which is the implementation of the inference process (which we covered [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6f6TTFnBFxj6"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    \n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "      \n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "        \n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6GuDf4TPWt6_"
   },
   "source": [
    "Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aAUntznIFxj9"
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9gM4ALyfWwA9"
   },
   "source": [
    "Here are a few summaries generated by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BUtQmQTmFxkI",
    "outputId": "f407d9fc-e0cd-4082-98f5-bd1f562dc26f"
   },
   "outputs": [],
   "source": [
    "for i in range(0,100):\n",
    "    print(\"Review:\",seq2text(x_tr[i]))\n",
    "    print(\"Original summary:\",seq2summary(y_tr[i]))\n",
    "    print(\"Predicted summary:\",decode_sequence(x_tr[i].reshape(1,max_text_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OTkaYNjHW4lC"
   },
   "source": [
    "This is really cool stuff. Even though the actual summary and the summary generated by our model do not match in terms of words, both of them are conveying the same meaning. Our model is able to generate a legible summary based on the context present in the text.\n",
    "\n",
    "This is how we can perform text summarization using deep learning concepts in Python.\n",
    "\n",
    "#How can we Improve the Model’s Performance Even Further?\n",
    "\n",
    "Your learning doesn’t stop here! There’s a lot more you can do to play around and experiment with the model:\n",
    "\n",
    "I recommend you to **increase the training dataset** size and build the model. The generalization capability of a deep learning model enhances with an increase in the training dataset size\n",
    "\n",
    "Try implementing **Bi-Directional LSTM** which is capable of capturing the context from both the directions and results in a better context vector\n",
    "\n",
    "Use the **beam search strategy** for decoding the test sequence instead of using the greedy approach (argmax)\n",
    "\n",
    "Evaluate the performance of your model based on the **BLEU score**\n",
    "\n",
    "Implement **pointer-generator networks** and **coverage mechanisms**\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_qIecuvY5GT"
   },
   "source": [
    "#End Notes\n",
    "\n",
    "If you have any feedback on this article or any doubts/queries, kindly share them in the comments section over [here](https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/) and I will get back to you. And make sure you experiment with the model we built here and share your results with me!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "How to build own text summarizer using deep learning.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
